import streamlit as st
import pandas as pd
import numpy as np
import faiss
import nltk
import re
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords

# Configuraci칩n inicial de la p치gina
st.set_page_config(page_title="Buscador de Papers Cient칤ficos", page_icon="游닄")
st.title("B칰squeda Sem치ntica en Art칤culos Cient칤ficos")
st.write("""
Busca art칤culos relevantes usando lenguaje natural. 
El sistema entender치 el contexto de tu b칰squeda y encontrar치 los papers m치s relevantes.
""")

# Descargar stopwords de NLTK
@st.cache_data
def descargar_stopwords():
    nltk.download('stopwords')
descargar_stopwords()
stop_words = set(stopwords.words('english'))

# Funci칩n para limpiar texto
def limpiar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'[^\w\s]', '', texto)
    texto = ' '.join([palabra for palabra in texto.split() if palabra not in stop_words])
    return texto

# Cargar modelo de Sentence Transformers
@st.cache_resource
def cargar_modelo():
    return SentenceTransformer('all-mpnet-base-v2')

modelo = cargar_modelo()

# Cargar y preprocesar datos
@st.cache_data
def cargar_datos():
    datos = pd.read_csv("data/articulos_cientificos.csv")
    datos['texto_completo'] = datos['abstract'] + " " + datos['full_text']
    datos['texto_limpio'] = datos['texto_completo'].apply(limpiar_texto)
    return datos

datos = cargar_datos()

# Generar embeddings y crear 칤ndice FAISS
@st.cache_resource
def preparar_sistema_busqueda(datos):
    textos = datos['texto_limpio'].tolist()
    embeddings = modelo.encode(textos)
    dimension = embeddings.shape[1]
    indice = faiss.IndexFlatL2(dimension)
    indice.add(embeddings)
    return embeddings, indice

embeddings, indice_faiss = preparar_sistema_busqueda(datos)

# Widget de b칰squeda
consulta = st.text_input("Escribe tu consulta cient칤fica:", 
                       placeholder="Ej: Avances recientes en terapia g칠nica para c치ncer...")

if consulta:
    with st.spinner('Buscando en m치s de 10,000 art칤culos...'):
        # Generar embedding para la consulta
        embedding_consulta = modelo.encode([consulta])
        
        # B칰squeda en el 칤ndice FAISS
        k = 10
        distancias, indices = indice_faiss.search(embedding_consulta, k)
        
        # Calcular similitud coseno
        embedding_plano = embedding_consulta.flatten()
        similitudes = []
        for idx in indices[0]:
            embedding_articulo = embeddings[idx]
            producto_punto = np.dot(embedding_plano, embedding_articulo)
            norma = np.linalg.norm(embedding_plano) * np.linalg.norm(embedding_articulo)
            similitudes.append(producto_punto / norma)
        
        # Crear DataFrame con resultados
        resultados = datos.iloc[indices[0]][['titulo', 'abstract', 'doi', 'a침o_publicacion']]
        resultados['Similitud'] = similitudes
        resultados['Distancia'] = distancias[0]
        
        # Formatear resultados
        resultados = resultados.sort_values('Similitud', ascending=False)
        resultados['Similitud'] = resultados['Similitud'].apply(lambda x: f"{x:.1%}")
        resultados['Distancia'] = resultados['Distancia'].apply(lambda x: f"{x:.2f}")

    # Mostrar resultados
    st.subheader(f"Top {k} resultados para: '{consulta}'")
    
    for _, fila in resultados.iterrows():
        with st.expander(f"游늯 {fila['titulo']} ({fila['a침o_publicacion']})"):
            st.markdown(f"**DOI:** `{fila['doi']}`")
            st.markdown(f"**Similitud:** {fila['Similitud']} | **Distancia:** {fila['Distancia']}")
            st.markdown("**Resumen:**")
            st.write(fila['abstract'])
            
    st.success(f"춰B칰squeda completada! Encontrados {len(resultados)} resultados relevantes.")
else:
    st.info("游눠 Escribe una consulta en el campo superior para comenzar tu b칰squeda.")

# Secci칩n adicional con informaci칩n del dataset
with st.sidebar:
    st.header("丘뙖잺 Acerca del sistema")
    st.markdown("""
    **Tecnolog칤as utilizadas:**
    - Modelo de embeddings: `all-mpnet-base-v2`
    - B칰squeda sem치ntica: FAISS
    - Procesamiento de texto: NLTK y RegEx
    
    **Caracter칤sticas del dataset:**
    - Art칤culos cient칤ficos de PubMed/PMC
    - Per칤odo: 2010-2023
    - Campos incluidos: t칤tulo, resumen, texto completo, DOI
    """)
    
    if st.checkbox("Mostrar metadatos del dataset"):
        st.write(f"游늵 Total de art칤culos: {len(datos):,}")
        st.write(f"游늰 Rango temporal: {datos['a침o_publicacion'].min()}-{datos['a침o_publicacion'].max()}")
        st.write("游 Campos disponibles:", ", ".join(datos.columns))
